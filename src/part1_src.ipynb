{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "5vAS-wbl2kMb",
    "outputId": "050702a7-260d-4d04-9f38-f298192cb9d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>En route , they pick up a seemingly-harmless h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jobs, however, are not created by supporting a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Due to circumstances , he is forced to abandon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I should just like to highlight two issues.\\tI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jerry is finally released from hospital and re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  En route , they pick up a seemingly-harmless h...\n",
       "1  Jobs, however, are not created by supporting a...\n",
       "2  Due to circumstances , he is forced to abandon...\n",
       "3  I should just like to highlight two issues.\\tI...\n",
       "4  Jerry is finally released from hospital and re..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "filename = \"train.txt\"\n",
    "with open(filename, 'r', encoding='utf-8',errors='ignore') as f:\n",
    "    data = f.read()\n",
    "\n",
    "df = pd.read_csv(pd.compat.StringIO(data),sep=\"\\r\\n\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nh_-Yu41DHe-"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "YOtxyOXH2kMf",
    "outputId": "14dcebe9-65a2-4716-bb63-51ffcf04db64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am absolutely convinced that if this body ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The competent Greek authorities have informed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This proves difficult , and is made more so by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On the way , Officer Satou and Takagi make a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shattered , she returns back and gets married ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  I am absolutely convinced that if this body ha...\n",
       "1  The competent Greek authorities have informed ...\n",
       "2  This proves difficult , and is made more so by...\n",
       "3  On the way , Officer Satou and Takagi make a d...\n",
       "4  Shattered , she returns back and gets married ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"test.rand.txt\"\n",
    "with open(filename, 'r', encoding='utf-8',errors='ignore') as f:\n",
    "    data = f.read()\n",
    "\n",
    "test_df = pd.read_csv(pd.compat.StringIO(data),sep=\"\\r\\n\", header=None)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hg-ZgHbV2kMi"
   },
   "outputs": [],
   "source": [
    "# test_df = test_df[:500]\n",
    "# test_df.head()\n",
    "# test_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ru3JStwj2kMl"
   },
   "outputs": [],
   "source": [
    "# df = df[:10000]\n",
    "# df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xE8YNU-f2kMn"
   },
   "outputs": [],
   "source": [
    "df['split'] = df[0].str.split('\\t')\n",
    "df['correct'] = df['split'].apply(lambda x: x[0])\n",
    "df['incorrect'] = df['split'].apply(lambda x: x[1])\n",
    "\n",
    "# training_set_df = pd.DataFrame(df['correct'])\n",
    "# training_set_df['label'] = 1\n",
    "# training_set_df1 = pd.DataFrame(df['incorrect'])\n",
    "# training_set_df1.columns=['correct']\n",
    "# training_set_df1['label'] = 0\n",
    "# training_set_df = training_set_df.append(training_set_df1,ignore_index=True)\n",
    "# X_train = training_set_df['correct']\n",
    "# y_train = training_set_df['label']\n",
    "# X_train.head()\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "training_list = []\n",
    "for index,row in df.iterrows():\n",
    "    text1 = nltk.word_tokenize(row['correct'])\n",
    "    tag1 = [tag[1] for tag in nltk.pos_tag(text1) ]\n",
    "    text2 = nltk.word_tokenize(row['incorrect'])\n",
    "    tag2 = [tag[1] for tag in nltk.pos_tag(text2)]   \n",
    "    training_list.append({'correct':row['correct'],'label':1, 'tag':tag1})\n",
    "    training_list.append({'correct':row['incorrect'],'label':0,'tag':tag2})\n",
    "#     training_list.append({'correct':row['correct'],'label':1})\n",
    "#     training_list.append({'correct':row['incorrect'],'label':0})\n",
    "\n",
    "training_df = pd.DataFrame(training_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sentences = training_df[training_df.index %2 == 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>En route , they pick up a seemingly-harmless h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jobs, however, are not created by supporting a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Due to circumstances , he is forced to abandon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I should just like to highlight two issues.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jerry is finally released from hospital and re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             correct  label\n",
       "0  En route , they pick up a seemingly-harmless h...      1\n",
       "2  Jobs, however, are not created by supporting a...      1\n",
       "4  Due to circumstances , he is forced to abandon...      1\n",
       "6        I should just like to highlight two issues.      1\n",
       "8  Jerry is finally released from hospital and re...      1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_sentence(s):\n",
    "    t = \"\"\n",
    "    for w in s:\n",
    "        t+=\" \"\n",
    "        t+=w\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xE8YNU-f2kMn"
   },
   "outputs": [],
   "source": [
    "# X_train = training_df['correct']\n",
    "X_train = training_df['tag'].apply(lambda x: convert_list_to_sentence(x) )\n",
    "y_train = training_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "numDKS1z2kMp",
    "outputId": "6a3b8a6e-a6d4-4029-8100-d6328920275c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           NNP NN , PRP VBP RP DT JJ NN , CC VB PRP$ NN ...\n",
       "1           NNP NN , PRP VBP RP DT JJ NN , CC VB PRP$ , V...\n",
       "2           NNP , RB , VBP RB VBN IN VBG DT JJ JJ NNS , W...\n",
       "3           NNP , RB , VBP RB VBN IN VBG DT JJ JJ NNS , W...\n",
       "4           JJ TO NNS , PRP VBZ VBN TO VB PRP$ NNS , IN P...\n",
       "5           JJ TO NNS , PRP VBZ VBN TO VB PRP$ NNS , IN P...\n",
       "6                                PRP MD RB VB TO VB CD NNS .\n",
       "7                                PRP MD RB VB TO VB CD NNS .\n",
       "8           NNP VBZ RB VBN IN NN CC NNS TO NNP TO VB IN N...\n",
       "9             NNP VBZ RB VBN IN NN CC NNS NNP TO VB IN NNP .\n",
       "10          IN DT NN , DT JJ NNP MD VB IN VBG JJ NNS RB ,...\n",
       "11          IN DT NN , DT JJ NNP MD VB IN VBG JJ NNS RB ,...\n",
       "12          NNP NNP , NNP NNP POS NN VBZ DT NN TO VB NN V...\n",
       "13          NNP NNP , NNP NNP POS NN VBZ DT NN TO VB NN V...\n",
       "14          DT JJ NN NN IN NNP VBZ VBN IN DT NN IN DT NN ...\n",
       "15          DT JJ NN NN IN NNP VBZ JJ IN DT NN IN DT NN NN .\n",
       "16                                           NNP CC NNP NN .\n",
       "17                                               NNP CC NN .\n",
       "18          DT JJ NN PRP$ VBN NN VBZ VBG DT NN CC VBZ DT ...\n",
       "19          DT JJ NN PRP$ VBN NN VBZ VBG DT NN CC VBZ DT ...\n",
       "20                   NNP VBZ TO VB IN PRP NNS , IN NNP NNS .\n",
       "21                   NNP VBZ TO VB IN PRP NNS , IN NNP NNS .\n",
       "22          JJ NN NN : JJ NNP NN NNP NN NNP CD NNP VBD NN...\n",
       "23          JJ NN NN : JJ NNP NN NNP NN NNP CD NNP VBD NN...\n",
       "24                   NN VBZ IN PRP VBZ JJ CC NNP VBZ JJ NN .\n",
       "25                   NN VBZ IN PRP VBZ JJ CC NNP VBZ JJ . NN\n",
       "26          RB , PRP MD VB TO VB NN IN NNP NN NN NNP NNP ...\n",
       "27          RB , PRP MD TO VB NN IN NNP NN NN NNP NNP VBD...\n",
       "28          IN IN DT , DT JJ NNP NNP VBZ NNP CC NNP CC VB...\n",
       "29          IN IN DT , DT JJ NN NNP VBZ NNP CC NNP CC VBZ...\n",
       "                                 ...                        \n",
       "1999970     IN PRP RB VBP RB JJ IN DT NN IN NN , IN DT NN...\n",
       "1999971     IN PRP RB VBP RB JJ IN DT NN IN NN , IN DT NN...\n",
       "1999972     IN DT NN , DT NNP VBZ RB VB JJ JJ NNS IN PRP$...\n",
       "1999973     IN DT NN , DT NNP VBZ RB JJ JJ JJ NNS IN PRP$...\n",
       "1999974                         NN VBZ DT JJS JJ NN IN NNP .\n",
       "1999975                          NN VBZ DT JJ JJ NN IN NNP .\n",
       "1999976     DT NNP NN IN DT NNP VBZ DT NNP , VBG IN DT JJ...\n",
       "1999977     DT NN IN DT NNP VBZ DT NNP , VBG IN DT JJ IN ...\n",
       "1999978     NNP VBZ PRP VBD DT NN NNS RB CC VBD VBN RP IN...\n",
       "1999979     NNP VBZ PRP VBD DT NN NNS RB CC VBD VBN RP IN...\n",
       "1999980     NNP RB VBZ PRP$ NNS IN NN IN DT NN TO VB RB V...\n",
       "1999981     NNP NN VBZ PRP$ NNS IN NN IN DT NN TO VB RB V...\n",
       "1999982                    RB , VB PRP VB PRP$ JJ NN IN NN .\n",
       "1999983                    RB , VB PRP VB PRP$ JJ NN IN NN .\n",
       "1999984                   IN NN , DT VBZ DT NN IN NN IN NN .\n",
       "1999985                   IN NN , EX VBZ DT NN IN NN IN NN .\n",
       "1999986                             PRP MD VB VBN IN DT NN .\n",
       "1999987                             PRP MD VB VBN IN JJ NN .\n",
       "1999988     PRP VBP VBN TO VB DT JJ NN IN DT NNP , WDT VB...\n",
       "1999989     PRP VBP VBN TO VB DT JJ NN IN DT NNP , WDT VB...\n",
       "1999990     NNP NNP , IN PRP VBP VBN TO VB , RB PRP MD VB...\n",
       "1999991     TO NNP , IN PRP VBP VBN TO VB , RB PRP MD VB ...\n",
       "1999992     PRP VBZ NNS IN DT CD NN NN TO NNP , VBG DT NN...\n",
       "1999993     PRP VBZ NNP IN DT CD NN NN TO NNP , VBG DT NN...\n",
       "1999994     NNP RB VBZ TO VB PRP$ JJ NN , VBG PRP VBP NN ...\n",
       "1999995     NNP . VBZ TO VB PRP$ JJ NN , VBG PRP VBP NN T...\n",
       "1999996       EX VBZ CD JJ NN WDT PRP MD VB TO VB IN DT NN .\n",
       "1999997          EX VBZ CD NN WDT PRP MD VB TO VB IN DT NN .\n",
       "1999998     NNP NNP , NNP , NNS CC NNS , DT NNP IN NNP NN...\n",
       "1999999     NNP NNP , NNP , NNS CC NNS , DT NNP IN NNP NN...\n",
       "Name: tag, Length: 2000000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "LGAslZLu2kMr",
    "outputId": "7de76e78-9e5f-473e-cf2c-882e21eb3c33"
   },
   "outputs": [],
   "source": [
    "test_df['split'] = test_df[0].str.split('\\t')\n",
    "test_df['correct'] = test_df['split'].apply(lambda x: x[0])\n",
    "test_df['incorrect'] = test_df['split'].apply(lambda x: x[1])\n",
    "# test_set_df = pd.DataFrame(test_df['correct'])\n",
    "# test_set_df1 = pd.DataFrame(test_df['incorrect'])\n",
    "# test_set_df1.columns=['correct']\n",
    "# test_set_df = test_set_df.append(test_set_df1,ignore_index=True)\n",
    "\n",
    "testing_list = []\n",
    "for index,row in test_df.iterrows():\n",
    "    text1 = nltk.word_tokenize(row['correct'])\n",
    "    tag1 = [tag[1] for tag in nltk.pos_tag(text1) ]\n",
    "    text2 = nltk.word_tokenize(row['incorrect'])\n",
    "    tag2 = [tag[1] for tag in nltk.pos_tag(text2)] \n",
    "    testing_list.append({'correct':row['correct'],'label':1,'tag':tag1})\n",
    "    testing_list.append({'correct':row['incorrect'],'label':0,'tag':tag2})\n",
    "\n",
    "test_set_df = pd.DataFrame(testing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "LGAslZLu2kMr",
    "outputId": "7de76e78-9e5f-473e-cf2c-882e21eb3c33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     PRP VBP RB VBN IN IN DT NN VBD VBN IN DT NN ,...\n",
       "1     PRP VBP RB VBN IN IN DT NN VBD VBN IN DT NN ,...\n",
       "2     DT NN NNP NNS VBP VBN PRP IN DT NN IN NNP NNP...\n",
       "3     DT NN NNP NNS VBP VBN PRP IN DT NN NNP NNP NN...\n",
       "4     DT VBZ JJ , CC VBZ VBN RBR RB IN JJ NNS VBG N...\n",
       "Name: tag, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test = test_set_df['correct']\n",
    "X_test = test_set_df['tag'].apply(lambda x: convert_list_to_sentence(x) )\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-M3Go6xKKPUy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "xqh2bkum2kMu",
    "outputId": "962f7a33-68ad-4c8f-81c4-066c209258a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000000x21524 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 39332242 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(3, 3)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "# X_validate_vectorized = vect.transform(X_validate)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awtfSMP42kMx"
   },
   "outputs": [],
   "source": [
    "# vect = Tfidf(ngram_range=(2, 3)).fit(X_train)\n",
    "# X_train_vectorized = vect.transform(X_train)\n",
    "# X_test_vectorized = vect.transform(X_test)\n",
    "# X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0w3xy-_j2kM0"
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# clf = LogisticRegression(C=100).fit(X_train_vectorized, y_train)\n",
    "# y_test = clf.predict(X_test_vectorized)\n",
    "\n",
    "# multinomialNBModel = MultinomialNB(alpha=0.1)\n",
    "# multinomialNBModel.fit(X_train_vectorized, y_train)\n",
    "# predictions = multinomialNBModel.predict(vect.transform(X_test))\n",
    "# print(\"here\")\n",
    "# len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bhkt09pa2kM4"
   },
   "outputs": [],
   "source": [
    "# clf = LogisticRegression().fit(X_train_vectorized, y_train)\n",
    "# predictions = clf.predict(X_test_vectorized)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aSTvPqpG2kM7"
   },
   "outputs": [],
   "source": [
    "# test_df['sent1']=predictions[:100000]\n",
    "# test_df['sent2']=predictions[100000:]\n",
    "\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "P3vSiKBG2kM_",
    "outputId": "3e2ee108-b3ff-4612-bfae-0bc23a1e31cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    " \n",
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    " \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    " \n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn max([len(s.split()) for s in lines])\n",
    " \n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded\n",
    " \n",
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinputs1 = Input(shape=(length,))\n",
    "\tembedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "\tconv1 = Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "\tdrop1 = Dropout(0.5)(conv1)\n",
    "\tpool1 = MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = Flatten()(pool1)\n",
    "\t# channel 2\n",
    "\tinputs2 = Input(shape=(length,))\n",
    "\tembedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "\tdrop2 = Dropout(0.5)(conv2)\n",
    "\tpool2 = MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = Flatten()(pool2)\n",
    "# \t# channel 3\n",
    "\tinputs3 = Input(shape=(length,))\n",
    "\tembedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "\tdrop3 = Dropout(0.5)(conv3)\n",
    "\tpool3 = MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = Flatten()(pool3)\n",
    "\t# merge\n",
    "\tmerged = concatenate([flat1, flat2, flat3])\n",
    "\t# interpretation\n",
    "#     dense1 = Dense(10, activation='relu')(flat1)\n",
    "\tdense1 = Dense(10, activation='relu')(merged)\n",
    "\toutputs = Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "#     model = Model(inputs=[inputs1], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "\tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "\treturn model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3_8Pa5zs2kNB"
   },
   "outputs": [],
   "source": [
    "# calculate vocabulary size\n",
    "vocab_size = len(vect.vocabulary_)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b56pe-e32kND"
   },
   "outputs": [],
   "source": [
    "# calculate max document length\n",
    "analyzer = vect.build_analyzer()\n",
    "\n",
    "def max_length_using_analyzer(lines):\n",
    "    return max([len(analyzer(s)) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFzdTwKxLM60"
   },
   "outputs": [],
   "source": [
    "length = max_length_using_analyzer(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IcpfIeJi2kNF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eP4kzpjx2kNH"
   },
   "outputs": [],
   "source": [
    "# load training dataset\n",
    "trainLines = X_train\n",
    "trainLabels = y_train\n",
    "ngrams = [analyzer(line) for line in trainLines]\n",
    "ngram_list = pd.Series(ngrams)\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(ngram_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3wQfync2kNJ"
   },
   "outputs": [],
   "source": [
    " \n",
    "# encode a list of lines\n",
    "# def encode_ngrams(tokenizer,analyzer, lines, length):\n",
    "# \t# integer encode\n",
    "#     ngrams = [analyzer(line) for line in lines]\n",
    "# \tencoded = tokenizer.texts_to_sequences(ngrams)\n",
    "# \t# pad encoded sequences\n",
    "# \tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "# \treturn padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "cFYoKFKB2kNL",
    "outputId": "6af11022-e040-4ce9-8277-6c8f515af5ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 670\n",
      "Vocabulary size: 21525\n",
      "(2000000, 670)\n"
     ]
    }
   ],
   "source": [
    "# calculate max document length\n",
    "length = max_length_using_analyzer(trainLines)\n",
    "# calculate vocabulary size\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, ngram_list, length)\n",
    "print(trainX.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "EZRhAGfQNP2p",
    "outputId": "cf799c2a-c085-4b86-ed62-440c71af73de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1140,  148, 1609, ...,    0,    0,    0],\n",
       "       [1140,  148, 1609, ...,    0,    0,    0],\n",
       "       [1855, 1305,  241, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [3618, 1729, 2694, ...,    0,    0,    0],\n",
       "       [  11,  369,  379, ...,    0,    0,    0],\n",
       "       [  11,  369,  379, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhTZFCT1aYeE"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "du3r7fjyKjX_",
    "outputId": "89059e66-39b5-43d8-ede6-b64d56c236d1"
   },
   "outputs": [],
   "source": [
    "# X_train_vectorized.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "colab_type": "code",
    "id": "Diik9obMad38",
    "outputId": "7a315897-5ab5-4fb5-8860-fcca07d164f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         2152500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,233,001\n",
      "Trainable params: 2,233,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "from keras import optimizers, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Activation, Lambda\n",
    "from tensorflow.contrib.opt import MomentumWOptimizer, AdamWOptimizer\n",
    "\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "tfoptimizer = AdamWOptimizer(weight_decay=0.1, learning_rate=0.001)\n",
    "options = {'weight_decay': 0.1, 'learning_rate': 0.001, 'momentum': 0.9, 'use_nesterov': True}\n",
    "\n",
    "\n",
    "# tfoptimizer = AdamWOptimizer(options)\n",
    "optimizer = optimizers.TFOptimizer(tfoptimizer)\n",
    "\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, 100))\n",
    "model2.add(LSTM(100))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "tfopt = AdamWOptimizer(weight_decay=0.1, learning_rate=.004)\n",
    "model2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# model.compile(loss='mse', optimizer=TFOptimizer(tf.train.GradientDescentOptimizer(0.1)))\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "HYdsOumqiZQH",
    "outputId": "e974ab70-9ead-468f-c06d-ace133124c70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2000000/2000000 [==============================] - 52309s 26ms/step - loss: 0.6931 - acc: 0.4998\n",
      "Epoch 2/2\n",
      "2000000/2000000 [==============================] - 53207s 27ms/step - loss: 0.6931 - acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a99661a0b8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(trainX, y_train, epochs=2, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "12YCkoi6b4_6"
   },
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49994832],\n",
       "       [0.49994832],\n",
       "       [0.49994832],\n",
       "       ...,\n",
       "       [0.49994832],\n",
       "       [0.49994832],\n",
       "       [0.49994832]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XTVdcL7ZOwK"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pNSeZ2Ny2kNR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 670)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 670)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 670)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 670, 100)     2152500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 670, 100)     2152500     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 670, 100)     2152500     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 667, 32)      12832       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 665, 32)      19232       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 663, 32)      25632       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 667, 32)      0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 665, 32)      0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 663, 32)      0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 333, 32)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 332, 32)      0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 331, 32)      0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10656)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 10624)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 10592)        0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 31872)        0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           318730      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,833,937\n",
      "Trainable params: 6,833,937\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # define model\n",
    "model = define_model(length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLJlqn9K2kNU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "2000000/2000000 [==============================] - 14307s 7ms/step - loss: 0.6797 - acc: 0.5698\n",
      "Epoch 2/2\n",
      "2000000/2000000 [==============================] - 14258s 7ms/step - loss: 0.6670 - acc: 0.5965\n"
     ]
    }
   ],
   "source": [
    "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=2, batch_size=2048)\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaYZAVsJ2kNY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 670)\n"
     ]
    }
   ],
   "source": [
    "# load training dataset\n",
    "testLines = X_test\n",
    "\n",
    "ngrams_test = [analyzer(line) for line in testLines]\n",
    "ngram_list_test = pd.Series(ngrams_test)\n",
    "# create tokenizer\n",
    "tokenizer1 = create_tokenizer(ngram_list_test)\n",
    "# encode data\n",
    "testX = encode_text(tokenizer1, ngram_list_test, length)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdawVguC2kNb"
   },
   "outputs": [],
   "source": [
    "# prediction = model.predict(testX, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNbuZUS92kNd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDFPoBjI2kNf"
   },
   "outputs": [],
   "source": [
    "prediction = model.predict([testX,testX,testX], batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2q7eCIGS2kNh"
   },
   "outputs": [],
   "source": [
    "# X_test.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGEShlyg2kNl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44093722],\n",
       "       [0.41597772],\n",
       "       [0.48048258],\n",
       "       ...,\n",
       "       [0.30585843],\n",
       "       [0.39817613],\n",
       "       [0.38043603]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kU_wqCo2kNm"
   },
   "outputs": [],
   "source": [
    "# def build_model(pr_hyper,m_hyper):\n",
    "    \n",
    "#     # Convolutional block\n",
    "#     model_input = Input(shape=(pr_hyper.sequence_length,),dtype='int32')\n",
    "#     # use a random embedding for the text\n",
    "#     x = Embedding(pr_hyper.max_words, m_hyper.embedding_dim)(model_input)\n",
    "#     x = SpatialDropout1D(m_hyper.dropout_prob[0])(x)\n",
    "\n",
    "#     conv_kern_reg = regularizers.l2(0.00001)\n",
    "#     conv_bias_reg = regularizers.l2(0.00001)\n",
    "    \n",
    "#     conv_blocks = []\n",
    "#     for sz in m_hyper.filter_sizes:\n",
    "#         conv = Convolution1D(filters=m_hyper.num_filters,\n",
    "#                              kernel_size=sz,\n",
    "#                              padding=\"same\",\n",
    "#                              activation=\"relu\",\n",
    "#                              strides=1,\n",
    "#                              kernel_regularizer=conv_kern_reg,\n",
    "#                              bias_regularizer=conv_bias_reg)(x)\n",
    "#         conv = GlobalMaxPooling1D()(conv)\n",
    "#         conv_blocks.append(conv)\n",
    "#     # merge\n",
    "#     x = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "    \n",
    "#     x = Dropout(m_hyper.dropout_prob[1])(x)\n",
    "#     x = Dense(m_hyper.hidden_dims, activation=\"relu\")(x)\n",
    "#     model_output = Dense(1, activation=\"sigmoid\")(x)\n",
    "#     model = Model(model_input, model_output)\n",
    "#     model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "#     print(model.summary())\n",
    "#     plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i2OqiOv12kNo"
   },
   "outputs": [],
   "source": [
    "# model2 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"prediction1.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "\n",
    "for i in prediction:\n",
    "#     dec = decimal.Decimal(i[0])\n",
    "    f.write(str(i[0])+\"\\r\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"prediction1.txt\"\n",
    "with open(filename, 'r', encoding='utf-8',errors='ignore') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.440937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.415978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.480483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.477830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.301003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.440937\n",
       "1  0.415978\n",
       "2  0.480483\n",
       "3  0.477830\n",
       "4  0.301003"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_values_df = df = pd.read_csv(pd.compat.StringIO(data),sep=\"\\r\\n\", header=None)\n",
    "result_values_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.get_backend()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.499948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.499948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.499948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.499948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.499948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0  0.499948\n",
       "1  0.499948\n",
       "2  0.499948\n",
       "3  0.499948\n",
       "4  0.499948"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"prediction2_lstm.txt\"\n",
    "with open(filename, 'r', encoding='utf-8',errors='ignore') as f:\n",
    "    data1 = f.read()\n",
    "    \n",
    "result_lstm_df = df = pd.read_csv(pd.compat.StringIO(data1),sep=\"\\r\\n\", header=None)\n",
    "result_lstm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lstm_list = result_lstm_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_cnn_list = result_values_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_list = len(result_cnn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1= open(\"result_cnn.txt\",\"w+\")\n",
    "\n",
    "for i in range(len_list):\n",
    "    if(i % 2 ==0):\n",
    "        if(result_cnn_list[i]>result_cnn_list[i+1]):\n",
    "            f1.write(str('A')+\"\\r\\n\")\n",
    "        else:\n",
    "            f1.write(str('B')+\"\\r\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "uwnlp.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
